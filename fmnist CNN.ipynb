{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist as fmnist\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid) = fmnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shapes the data and converts it to a float for the flattening stage. The 60,000 refers to the number of samples in the fmnist dataset. The two 28's refer to the width and height in pixels of the images in the data. The 1 in the last slot refers to the number of color channels and since these are greyscaled we only have 1 color channel. Remember we want to keep these images as 2 dimension becasue we are using a trasnfer learning model vgg19 which is a pretrained CNN. It needs to be given 2 dimenison to make use of it's spatial recognition ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 28, 28, 1).astype('float32')\n",
    "X_valid = X_valid.reshape(10000, 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the points between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_valid /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 different articles of clothing in the data set. Set the labels to categorical as we are attempting to classify a categorical element (article of clothing is not continous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, 3, activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                802880    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 822,602\n",
      "Trainable params: 822,474\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 45s 95ms/step - loss: 0.3115 - accuracy: 0.8894 - val_loss: 0.2979 - val_accuracy: 0.8908\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 43s 93ms/step - loss: 0.2585 - accuracy: 0.9058 - val_loss: 0.2377 - val_accuracy: 0.9166\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 45s 95ms/step - loss: 0.2291 - accuracy: 0.9163 - val_loss: 0.2453 - val_accuracy: 0.9116\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 44s 95ms/step - loss: 0.2028 - accuracy: 0.9257 - val_loss: 0.2383 - val_accuracy: 0.9156\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.1867 - accuracy: 0.9293 - val_loss: 0.2418 - val_accuracy: 0.9169\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.1680 - accuracy: 0.9371 - val_loss: 0.2468 - val_accuracy: 0.9231\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.1558 - accuracy: 0.9415 - val_loss: 0.2899 - val_accuracy: 0.9171\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 46s 97ms/step - loss: 0.1476 - accuracy: 0.9436 - val_loss: 0.2663 - val_accuracy: 0.9258\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.1404 - accuracy: 0.9468 - val_loss: 0.2677 - val_accuracy: 0.9242\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.1235 - accuracy: 0.9518 - val_loss: 0.2667 - val_accuracy: 0.9257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de827894c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComVis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
